{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d1da7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Debdutta Chatterjee\\Work\\Project\\Python\\AI_ML_DS_GenAI_Portfolio\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model = 'models/text-embedding-004',\n",
    "   google_api_key ='AIzaSyCtTc-rIeCTfJsfIMHFqnSIjhPbSJpy5Yc'\n",
    ")\n",
    "\n",
    "embeddings\n",
    "\n",
    "\n",
    "text=\"Hello, I am learning about embeddings!\"\n",
    "em = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4f558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869c8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9669f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = FAISS.from_documents(docs,embeddings)\n",
    "dense_retriever = vs.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981f42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ad5f5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001DA72B4B8F0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001DAB35D8680>, k=5)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retiever = EnsembleRetriever(\n",
    "    retrievers =[dense_retriever,sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")\n",
    "hybrid_retiever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c9aa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ac5acb2e-615f-4ded-b7ef-bbea22a7024a', metadata={}, page_content='LangChain helps build LLM applications.'),\n",
       " Document(id='15711ebc-f18c-4e46-bb37-e78b1b29e3b9', metadata={}, page_content='Langchain can be used to develop agentic ai application.'),\n",
       " Document(id='62d185ce-f6a1-4fba-b001-2a41162a98d7', metadata={}, page_content='Pinecone is a vector database for semantic search.'),\n",
       " Document(id='6c625c9b-5ff1-4454-9ba8-2f1230809871', metadata={}, page_content='Langchain has many types of retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retiever.invoke(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f25156a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--e7c2ae1a-a7ef-4ce4-8e96-54df45f7869c-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "                model='gemini-2.0-flash',\n",
    "                google_api_key=\"AIzaSyCtTc-rIeCTfJsfIMHFqnSIjhPbSJpy5Yc\",\n",
    "                temperature=0,\n",
    "                max_output_tokens=1000\n",
    "            )\n",
    "\n",
    "model.invoke('Hello')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e81959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725c49a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001DA72B4B8F0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001DAB35D8680>, k=5)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_output_tokens=1000, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001DAB3778830>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff=create_stuff_documents_chain(\n",
    "    model,prompt\n",
    ")\n",
    "rag_chain = create_retrieval_chain(hybrid_retiever,combine_docs_chain=stuff)\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90046dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " Based on the context, you can build an app using LLMs with the help of **LangChain**. LangChain is a framework specifically designed for building LLM applications and can be used to develop agentic AI applications.\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(\"✅ Answer:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e31536b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vs.as_retriever(search_type='mmr',\n",
    "                            kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f58c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001DA72B4B8F0>, search_type='mmr', search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_output_tokens=1000, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001DAB3778830>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "stuff = create_stuff_documents_chain(model,prompt)\n",
    "rag=create_retrieval_chain(retriever,combine_docs_chain=stuff)\n",
    "rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28f159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, \"langchain\" refers to a framework that helps build LLM applications.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.invoke({'input':'langchain'})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce542074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "\n",
    "# Split text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea1b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "vs = FAISS.from_documents(docs,embeddings)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\":8})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents \n",
    "from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50add758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a41657e2-cb31-4429-87a8-bf14c3bd2620', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='a72e56aa-4f97-491f-af51-5b1801ba65d6', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='5e90eb66-64a7-430b-8ce7-ca1aaab247bc', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='ea9f1ffd-f81d-44c3-9b67-26c6f1268252', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='487ac420-94a4-4f23-933d-7460aeed3e7d', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='6b9a07ff-2a52-4857-843d-4430c5686f7d', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"How can i use langchain to build an application with memory and tools?\"\n",
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e99d1d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2, 1, 4, 0, 3, 5'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "lcel = prompt|model|StrOutputParser()\n",
    "\n",
    "response = lcel.invoke({\"question\":query,\"documents\":retrieved_docs})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
